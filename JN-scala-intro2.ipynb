{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://127.0.0.1:4040\n",
       "SparkContext available as 'sc' (version = 2.2.0, master = local[*], app id = local-1507769422791)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "add: (x1: Int, x2: Int)Int\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add(x1:Int,x2:Int): Int = \n",
    "{x1+x2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Int = 3\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Int = 15\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "List(1,2,3,4,5).sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list: List[Int] = List(1, 2, 3, 4, 5)\n",
       "sum: Int = 15\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val list = List(1,2,3,4,5)\n",
    "val sum = list.foldRight(0)((item, acc) => item + acc) // similar to reduce "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "up: (str: String)Boolean\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def up(str: String): Boolean = {\n",
    "    str.exists(letter => letter.isUpper)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res: Boolean = true\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val res = up(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res: Boolean = false\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val res = up(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "12: error: class List needs to be abstract, since:",
     "output_type": "error",
     "traceback": [
      "<console>:12: error: class List needs to be abstract, since:",
      "it has 2 unimplemented members.",
      "/** As seen from class List, the missing signatures are as follows.",
      " *  For convenience, these are usable as stub implementations.",
      " */",
      "  def flatMap[A, B](f: A => B): List[B] = ???",
      "  def map[A, B](f: A => B): List[B] = ???",
      "",
      "       class List[A] { // generic [A]",
      "             ^",
      ""
     ]
    }
   ],
   "source": [
    "class List[A] { // generic [A]\n",
    "    def map[A, B](f: A => B): List[B]\n",
    "    def flatMap[A, B](f: A => B): List[B]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@7b22451f\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Â ,10)\n",
      "($,8)\n",
      "(SCREEN,4)\n",
      "(INSTANCE,4)\n",
      "(TO,4)\n",
      "(APP,3)\n",
      "(USE,3)\n",
      "((TO,3)\n",
      "(WEB,3)\n",
      "(EC2,2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fileRDD: org.apache.spark.sql.Dataset[String] = [value: string]\n",
       "r: org.apache.spark.sql.Dataset[(String, Long)] = [value: string, count(1): bigint]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fileRDD = spark.read.textFile(\"README.md\")\n",
    "val r = fileRDD.flatMap(_.split( \" \")) // one list of all instead of list of lists\n",
    "                .filter(!_.isEmpty)                // filter out empty strings\n",
    "                .map(word => word.toUpperCase)     // convert all characters to uppercase\n",
    "                .groupByKey(x => x)                // list[string]\n",
    "                .count()                           // aggregate function\n",
    "r.collect().sortBy(-_._2).take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
